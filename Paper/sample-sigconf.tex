\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables

\newcommand{\todo}[1]
  {{\scriptsize \textbf{\color{red} {#1}}}}


% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}



% DOI
\acmDOI{10.475/123_4}

% ISBN
\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[MSR'18]{Mining Software Repositories}{May 2018}{Gothenburg, Sweden}
\acmYear{2018}
\copyrightyear{2018}


\begin{document}
\title{Common Statement Kind Changes and their Application in Automatic Program Repair}
%\titlenote{Produces the permission block, and
%  copyright information}
%\subtitle{Extended Abstract}
%\subtitlenote{The full version of the author's guide is available as
%  \texttt{acmart.pdf} document}


\author{Mauricio Soto}
\affiliation{%
  \institution{Carnegie Mellon University}
  \city{Pittsburgh}
  \state{PA}
  \postcode{15213}
}
\email{mauriciosoto@cmu.edu}

\author{Claire Le Goues}
\affiliation{%
  \institution{Carnegie Mellon University}
  \city{Pittsburgh}
  \state{PA}
  \postcode{15213}
}
\email{clegoues@cs.cmu.edu}


% The default list of authors is too long for headers.
%\renewcommand{\shortauthors}{B. Trovato et al.}


\begin{abstract}
The search space for automatic program repair approaches 
is vast and the search for mechanisms to help restrict this 
search are increasing.
We make a granular analysis based on statement kinds
to find which statements are more likely to be modified than others
when fixing an error.
We construct a corpus for analysis by 
delimiting debugging regions in the provided dataset and 
recursively analyze the
differences between the SST's associatiated with EditEvent's. 
We build a distribution of statement kinds with their corresponding
likelihood of being modified and we validate the usage of this 
distribution to guide the statement selection.
We then build association rules with different
confidence thresholds to describe statement kinds commonly modified
together for multi-edit patch creation. Finally
we evaluate association rule coverage over a held out test set
and find that when using a 95\% confidence threshold we
can create less and more accurate rules that fully cover 93.8\% of the testing instances.

\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011074.10011111.10011113</concept_id>
<concept_desc>Software and its engineering~Software evolution</concept_desc>
<concept_significance>100</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[100]{Software and its engineering~Software evolution}

\maketitle

\section{Introduction}
\todo{Kill the author block; it's double-blind.}
\todo{Title is too long and somewhat innaccurate; let's develop something better.}
Bug repair is one of the most resource consuming tasks in 
software development~\cite{Weiss07,Tassey02,Britton13}.
In the last decade there has been increasing attention to 
techniques for automatic program repair (APR), which 
repair software bugs
automatically (e.g.~\cite{legoues12,kim2013,Weimer13,long16proph}).
One family of techniques follows a syntactic \emph{generate-and-validate}
approach (e.g.~\cite{legoues12,kim2013,Qi13TrpAutoR,xuan16}). These techniques
take as input a buggy program and a test suite containing
passing tests cases and failing test cases (which expose a defect).
The approaches then \emph{generate} 
patch candidates by applying and combining syntactic source transformations, and \emph{validate} these 
candidates against the test suite.

One challenge in in this process is that the search space
of patch candidates is trivially infinite.  APR approaches thus restrict the
considered transformations, and use off-the-shelf fault
localization~\cite{example} to reduce the number of locations considered for
editing. One problem is that 
statistical fault localization can often assign multiple statements
the same score~\cite{ref}.  \todo{...multi-edit repair? I know it's not all we
  do, but now the whole thread has been lost.  Add an argument here, motivate
  multi-edit repair.  That's the real bread-and-butter of this thing.}


We believe this selection could be improved by using
developer's
edit history. \todo{Our belief is not a great motivation.  There's evidence here
  from prior work (including our own, that we can cite).  Make this more
  scientific.} We therefore analyze the most commonly 
modified statement kinds and evaluate how the 
fault localization process can benefit from assigning
a higher priority to the statements most commonly modified.

\todo{This introduction now motivates only the non-multi-edit results but only
  describes the multi-edit experiment.  It needs to motivate both and describe
  both.}

We mined changes performed by developers in the debugging
process by delimiting debugging regions in the 
dataset~\cite{msr18challenge} and comparing the simplified syntax
trees (SST) of consecutive \textit{EditEvent's} using an 
state-of-the-art tree differencing tool~\cite{Pawlik16Apted}.\todo{Need to
  introduce the dataset with like, a half a sentence, before referring to it here.}
We then gather the statement kinds edited when
differencing SST's and apply the association
rule learning algorithm Apriori~\cite{Agrawal94}
to obtain relationships that describe what statement kinds
are edited commonly together. This knowledge helps to 
bound the search space for automatic program repair approaches
in the effort of building multi-edit patches.

We evaluate the level to which the rules can predict edits
by analyzing what sections of the edits are covered by the 
learned association rules. 
Finally we analyze
the confidence level of the learned rules to a more granular 
level than performed before in the context of automatic
program repair. We found that when we use a 95\% confidence
threshold, we can create association
rules that maintain the same flexibility as previous
studies with lower confidence thersholds\cite{Soto18}, but increases
the accuracy of the created rules and decreases the number of rules
delimiting even more the search space of potential edits
performed by APR approaches to obtain successful patches.

The main contributions of this paper are the following:
\begin{itemize}
\item We \textbf{describe the distribution} of statement kinds being
modified by developers when fixing errors in source code. 

\item We \textbf{evaluate
the selection process} of statement kinds to be modified
to fix source code errors, using history-based data, compared against
an equally distributed random approach.

\item We \textbf{create association rules} that describe the behavior
of statement kinds that developers modify commonly together.

\item We provide a \textbf{higher granularity evaluation} 
of confidence thresholds when creating
association rules in the context of APR.\todo{I'm not sure what this means,
  perhaps a rephrase?}

\end{itemize}

\todo{On some or all of teh above: what are the results?  You describe what we
  did, but not what we learn.}

\section{Creating the Corpus}

We collect a corpus of events based in modifications 
developers perform when fixing an error. We then
use this corpus to create a distribution of statement
kinds modified and association rules.\todo{reference the challenge corpus in a citation}

\subsection{Delimiting Debugging Regions}
\label{delimitDebugRegions}


The first step towards analyzing the debugging
process is identifying periods during which a 
developer is trying to fix a bug.
We delimit the debugging regions by starting at a point
in time $\delta$ where a \textit{TestRunEvent} is triggered and 
all test cases were run (i.e., test case execution
was not aborted) but one or more test cases failed, while in the 
previous \textit{TestRunEvent}, all tests passed.
The delimitation ends at a point in time $\epsilon $ where $ \epsilon > \delta$ 
and where all test cases were ran and passed.\todo{and another test run event
  happened first?}

Figure~\ref{demarcations} shows an example of a time line
with only the \textit{TestRunEvent}s shown. The 
horizontal arrow represents a timeline showing
the events during which a test suite was executed. ``P" represents a case
where all test cases passed; ``F" represents a case where one
or more test cases failed. The \texttt{debugging areas} are shown by
the brackets demarcated with ``DA". 
Note that between delimiting points $\delta$ and $\epsilon$ there can be 
several \textit{TestRunEvent's} with failing test cases, meaning
that the developer is actively debugging the error pointed out
by the failing test cases, but still hasn't been able to make
all test cases pass.

\begin{figure}[h]
\caption{Example of the process used to obtain the demarcation of
debugging areas. Events where all test cases pass (P) and
events where one or more tests fail (F) are analyzed to create
debugging areas (DA).}
\centering
\includegraphics[width=0.5\textwidth]{images/demarcations.png}
\label{demarcations}
\end{figure}

We identified 634 debugging sequences in the dataset 
provided by the MSR Challenge~\cite{msr18challenge}.
Within these debugging sequences, there are 1,251,334
different\todo{unique?} events, from which 1,748 are \textit{EditEvent's}
that have associated a not-unknown Contexts, and therefore 
contain a valid Simplified Syntax Tree.\todo{What is a Simplified
  Syntax Tree?  Introduce this in the front part of the section, overviewing the
approach} These \textit{EditEvent's} 
are our primary subject of analysis.

\subsection{Simplified Syntax Tree Differencing}
\label{sstDiff}

Given two consecutive \textit{EditEvent's} within
a debugging region, we procede to analyze the differences
between their two associated Simplified Syntax Trees (SST).
These correspond to the modifications the developer performed
while debugging.
We use the 
state-of-the-art tree differencing
tool APTED~\cite{Pawlik16Apted}, recursively converting the SST's into a
representation readable by APTED. 
Once we know which nodes were modified between \textit{EditEvent}s, we record
their corresponding statement types. 

\subsection{Statement Kind Distribution and Association Rule Creation}

We analyzed the distribution of statement kinds modified
in the corpus.  EditEvent's may modify more than one
statement kind, therefore the sum of these distribution
does not sum to 100\%.
From our 1,748 events, the most commonly modified statement
kinds are ExpressionStatements, modified in 1,272  (72.8\%) corpus events,
followed by Assignment's modified in 1,186 
events (67.8\%). VariableDeclaration's are modified in 1,071
events (61.3\%) and ReturnStatement's are modified in 714
events (40.8\%).\footnote{The full list of Statement kind
distrubtion of the corpus is located in the anonymouos folder 
https://www.dropbox.com/sh/lei59ywj1ok7gfk/AACc7JyJ-BcCa9-Kt1BqzGWja?dl=0} 
\todo{Takeaway from this?  Implication for APR?  One sentence summary of the
  high level point.}


%Table~\ref{corpusDist} shows the top 10 most commonly modified 
%statement kinds
%in the corpus. The "Statement Kind" column shows the name of the 
%Statement kind being modified, the "Instance Precense" column
%shows the number of instances that modify the corresponding
%statement kind, and the "Percentage Presence" column shows 
%how many instances in the context of the entire corpus of 
%1,748 edits found in the debuggin areas modify the corresponding
%statement kind. For example, the first row states that
%an "ExpressionStatement" is modified in 1,272 EditEvent's from the
%1,748 that make up the entire corpus, therefore  it is modified
%in a 72,8\% of the corpus. EditEvent's may modify more than one
%statement kind.

%\begin{table}[]
%\centering
%\caption{Distribution of top 10 statement kinds modified by 
%developers when fixing an error in source code. }
%\label{corpusDist}
%\begin{tabular}{lrr}
%\toprule
%Statement Kind      & Instance Precense & Percentage Presence  \\
%\midrule
%ExpressionStatement & 1272 & 72.8\% \\
%Assignment          & 1186 & 67.8\% \\
%VariableDeclaration & 1071 & 61.3\% \\
%ReturnStatement     & 714  & 40.8\% \\
%IfElseBlock         & 657  & 37.6\% \\
%ForEachLoop         & 334  & 19.1\% \\
%ThrowStatement      & 181  & 10.4\% \\
%TryBlock            & 180  & 10.3\% \\
%UsingBlock          & 103  & 5.9\%  \\
%WhileLoop           & 86   & 4.9\%  \\
%\bottomrule
%\end{tabular}
%\end{table}

To collect rules for statement kinds that are modified together in
multi-edit patches, we use Apriori~\cite{Agrawal94} to create
association rules.
Association rule learning is a machine learning mechanism that identifies
relationships between objects in large datasets. 
Association rules are implications of the
form $X \implies Y$.
% antecedent->conequent is a known thing from logic
This learning mechanism takes into 
account \textit{Confidence} (a measure of the likelihood 
of the consequent section being present given the 
antecedent) and \textit{Support} (how frequently the set of
statement types in the rule occurs in the corpus).

Using our corpus of edits and their associated statement types, 
we create association rules that provide guidelines
for what statement types are modified together commonly. 

\section{Evaluation}
\label{eval}

We evaluate the modified statement kind selection process
by comparing a random selection against our History-based 
approach which gives higher priority to more commonly 
modified statement kinds. We then evaluate the association
rules by measuring the percentage of edit instances they correctly predict, or 
cover.

\subsection{History-Based Statement Selection}

To evaluate the distribution of the statement kinds modified
in the corpus, we compare against an equally distributed counterpart.
This simulates the process being used by most current APR approaches
where blocks of code are analyzed using a fault localization 
\todo{This sentence is not grammatical, please fix.}
mechanism and randomly selecting an statement from the most
suspicious block.

We use 10 fold
cross validation to avoid training and testing on the same data
when evaluating a selection bounded by the distribution
\todo{this sentence is not parsing for me, I'm afraid.  Perhaps simplify it.}
learned. For each of the folds, we
iterate through each EditEvent. For
the equally distributed approach, we randomly select one from 
all possible statement kinds and check if the selected
statement kind was modified in the EditEvent.
For our history-informed approach, we create a distribution
of the statement kinds modified from the complement of the fold, 
and select a statement
kind bounded by the associated distribution.
The more commonly modified statement
kinds are more likely to be chosen than the less commonly modified
statement kinds.

Table~\ref{10FoldEDvsHB} describes the results per each evaluated fold.
On average, 
the History-based approach is able to correctly guess one of the statement kinds
49.54\%. The Equally distributed approach
on average is only able to correctly guess 5.11\% of the time.
Two sample t-test indicates that the difference between the Equally distributed
and History-based approaches is statistically significant ($\alpha$ < 0.05).


\begin{table}[]
\centering
\caption{Evaluation of modified statement kind selection (Equally distributed vs History-based).}
\label{10FoldEDvsHB}
\begin{tabular}{l|rr|rr}
\toprule
   &   \multicolumn{2}{c|}{ Equally Distributed}   &   \multicolumn{2}{|c}{ History-Based} \\
\midrule
Fold  &   Count & Percentage  &  Count & Percentage  \\
\midrule
1&13&(7.39\%) & 72&(40.90\%) \\
2&4&(2.27\%) & 98&(55.68\%) \\
3&11&(6.25\%) & 86&(48.86\%) \\
4&7&(3.98\%) & 84&(47.72\%) \\
5&10&(5.68\%) & 92&(52.27\%) \\
6&7&(3.98\%) & 89&(50.57\%) \\
7&10&(5.68\%) & 92&(52.27\%) \\
8&8&(4.54\%) & 89&(50.57\%) \\
9&11&(6.25\%) & 96&(54.54\%) \\
10&9&(5.11\%) & 74&(42.04\%) \\
\bottomrule
Mean &9 & 5.11 & 87.2 & 49.54 \\
\bottomrule
Std Dev & 2.45 & 1.39 & 8.15 & 4.43 \\
\bottomrule

\end{tabular}
\end{table}



\subsection{Association Rule Coverage}

We next evaluate the effectiveness
of the association rules and their potential for use in building the
necesary edits to create successful multi-edit patches. We first
remove from our corpus the instances where the developers
performed a single edit.\todo{how many?} This is because association rules
by definition require at least two elements: an antecedent
and a consequent. 

We then analyze how often the association rules can build the 
modifications in the events by using rule coverage. We use 10 fold cross validation to avoid training and testing in overlapping data.
First, we divide our corpus into 10 folds.
For each of the folds we perform the following actions:
the selected fold is used as testing data, and the
remaining nine folds are used to create association rules.
We then analyze how many of the events in the testing data
can be built from the association 
rules created from the remaining nine folds.
The learned rules can either cover all the edits in an event (the 
event is \emph{fully covered}), they can cover some of the edits in an 
event (the event is \emph{partially covered}), or they can cover none of
the edits in an event (the event is \emph{not covered}).

We show an example for further understanding of the coverage concept.
Table~\ref{rulesandinstances} shows three events. In 
our example Event~1 describes an event
where the developer modified an Assignment, a ForLoop and an IfElseBlock.
The rules shown in Table~\ref{rulesandinstances} represent rules created
from the remaining nine folds of the corpus. In our example: Rule~1 
indicates that when an Assignment and a ForLoop have been modified,
the next step is to modify an \texttt{IfElseBlock}.\todo{I added the tt tag
  around the IfElseBlock; please do the same around other instances of statement
  kind descriptions througout (like assignment, forloop, etc).  Use typesetting
  to help your reader parse.}


\begin{table}[ht]
  \centering
  \caption{Events (top); learned association rules (bottom). \label{rulesandinstances}}{\small
\begin{tabular}{ll}
\toprule
\multicolumn{2}{c}{Events} \\
\midrule
1 & Assignment; ForLoop; IfElseBlock  \\
2 & Assignment; ForLoop; VariableDeclaration\\  
3 & Assignment; ForLoop;  ReturnStmt \\
\midrule
 \multicolumn{2}{c}{Association Rules} \\                     
\midrule
1 & Assignment $\wedge$ ForLoop $\rightarrow$ IfElseBlock \\  
2 & ForLoop $\rightarrow$ VariableDeclaration \\   
\bottomrule
\end{tabular}
}
\end{table}

In this example, Event~1 is fully covered since Rule~1 covers all
the edits in the event. For Event~2, Rule~1 does not apply, even though
Event~2 contains the antecedent of Rule~1, it does not contain the 
concequent, therefore the rule is discarded. Rule~2 does apply to 
cover the latter two
edits of Event~2,
therefore Event~2 is partially covered. Finally Event~3 is not covered,
even when some of its edits are contained in the antecedent of the rules,
there is no rule that would successfully predict the behavior of this 
event. 

Finally we performed this analysis at 3 different confidence thresholds
(90\%, 95\%, 100\%) to compare the accuracy of the rules against the 
number of rules that can be created with lower confidence. 
Higher confidence produces a small set of very
accurate rules, in which it is very likely that when the antecedent
is present the concequent will be present as well. But this approach
does not allow for much flexibility and the rules overfit to the corpus 
they were created from. Lower 
confidence creates a wide set of rules that are less accurate (if
the antecedent is present is does not necessarily mean that the 
consequent will be present as well) but more flexible. 

% CLG: I cut the reference to your prior paper because you used it broadly as
% though it were talking about all a priori learning and it's not, it's just
% about one use case (and not this one.)
For each confidence
threshold of 90\%, 95\%, and 100\%, we evaluated association rule coverage on a held out
dataset, using 
a standard 10 fold cross validation process. Finally, we aggregate the results
from all folds.

Figure~\ref{ruleEvaluation} shows our results for each of the 
different confidence thresholds as described below.

\vspace{1em}
\noindent\textbf{90\% confidence threshold:}
When evaluating the rules created under a 90\% threshold, 94.14\% of the 
events are fully covered, 1.87\% are partially covered, and 3.99\% are
not covered. When using this approach, an average of 2837.2 rules were
created for the 10 folds.

\vspace{1em}
\noindent\textbf{95\% confidence threshold:}
93.80\% of the testing instances are fully covered, 2.04\% are
partially covered, and 4.9\% are not covered. On, average 2134.3 rules
were created.

\vspace{1em}
\noindent\textbf{100\% confidence threshold:}
Finally, 35.85\% are fully
covered, 2.29\% are partiallly covered, and 72.80\% are not covered.
On average, 1484.6 rules were created when using this approach.

\vspace{1em}
We found that when creating association
rules with empirical data for automatic program repair, 95\% confidence appears
idea: 
it maintains very similar flexibility 
to the 90\% threshold with improved accuracy.
This also means that the number of rules created decreases, which helpfully
restricts 
the search space by restricting it to a smaller number of more accurate rules.
 

\begin{figure}[h]
\caption{The wide bars are described by the left Y axis. This represents
the percentage of instances covered
fully, partially or not by the association rules (higher full coverage is better). 
The thin bars are described by the right Y axis.
This represents the number of rules created for each confidence threshold 
(lower is better).
The bars from left to right describe the cross validation
results when using 90\%, 95\% and 100\% confidence thresholds correspondingly.}
\centering
\includegraphics[width=0.5\textwidth]{images/assocRuleEval.png}
\label{ruleEvaluation}
\end{figure}

\section{Threats to Validity / Related Work} \label{threatsVal}

\todo{You can't just munge threats and related work together.  They're unrelated.  Put threats in a
  subsection in the previous section and related work in its own section.}

\noindent\textbf{Internal validity:}
To tackle possible errors in our implementation and experiments, we release our code
to be reviewed by the other researchers.\footnote{All instruments
and source code can be found in the anonymouos folder 
https://www.dropbox.com/sh/lei59ywj1ok7gfk/AACc7JyJ-BcCa9-Kt1BqzGWja?dl=0} 
We also use 10 fold cross validation 
to reduce the risk of training and testing on the same data.  

\noindent\textbf{External validity:} 
It is possible 
that our results will not generalize to external datasets and to
real bug fixes. To mitigate this concern, we have created our corpus 
from the dataset made available to us by the MSR Challenge~\cite{msr18challenge}
which records the steps developers take while in the software development process.
This dataset is gathered from a diverse pool of volunteers with different 
backgrounds and levels of expertise.

\noindent\textbf{Related Work:} 
Soto et al.~\cite{Soto18} have created association rules to inform the APR
process basing their corpus in \textbf{mutation operators} taken from popular Github projects
written in Java using commit level granularity, 
different from our approach which creates association rules for \textbf{statement types} 
based in finer grained C\# code changes~\cite{msr18challenge}. Mutation operators help to 
inform what \textit{action} to perform next, while statement types help to inform what
object to modify next. 
Martinez and
Monperrus~\cite{martinez15} study mutation operator incidence across 
14 projects. Par~\cite{kim2013} describes an analysis of common changes
applied by humans when fixing errors and templates that can be mined from
their corpus. HDRepair~\cite{xuan16} 
uses fix history at a broader level
to assess patch suitability.
\todo{Add the paper from ...someone and Su on empirical study of debugging (ICSE
  '15).  Also, you very very obviously need to cite Prophet.  We're obviously
  not proposing a new APR technqiue (which you should say), but you definitely
  need to cite prophet.  Check your own SANER paper, there are at least a couple
of very related pieces of work to add.}

\section{Conclusions}
\label{conclusions}

In this study we mined the dataset provided by the MSR Challenge~\cite{msr18challenge}
to create a corpus of highly granular edits performed by developers in the process
of fixing an error in source code. We identify debugging
regions and inspect the changes between the simplified 
syntax trees from each
of the \textit{EditEvent's} to create a corpus of 
events.
We analyze the distribution of commonly modified statement kinds and evaluate
how a search process bounded by this history can correctly guess (49.54\% of 
cases) what statement kind is modified to fix an error, as opposed to its
equally distributed counterpart (5.11\% of the cases).

We then create association rules toprovide guidelines of what kinds of
statement to edit together to create successful multi-edit patches. 
We measure how many of the events
can be fully, partially, or not covered by the association rules; we find that
95\% appears to be the preferred condifence threshhold.
We are able to fully cover 93.80\% of the events with a 95\% confidence threshold
which increases the accuracy of the rules created and decreases the 
number of rules, therefore helping delimit the vast search space 
in automatic program repair.\todo{Sentence about future work, wherein we will
  presumably use these models to inform multi-edit repair.}


%Below are shown the top 10 rules created with a 95\% confidence, a full list can be 
%found online\footnote{the link will be made available after double blind
%review since it may uncover the identity of the authors}:
%\begin{itemize}
% \item ReturnStatement \& VariableDeclaration $\implies$ Assignment
% \item IfElseBlock \& VariableDeclaration $\implies$ Assignment
% \item ExpressionStatement \& VariableDeclaration $\implies$ Assignment
% \item VariableDeclaration $\implies$ Assignment
% \item IfElseBlock \& ExpressionStatement $\implies$ Assignment
% \item ExpressionStatement \& ReturnStatement $\implies$ Assignment
% \item IfElseBlock $\implies$ Assignment
% \item ReturnStatement $\implies$ Assignment
% \item IfElseBlock \& Assignment $\implies$ VariableDeclaration
% \item Assignment \& ReturnStatement $\implies$ VariableDeclaration
% \end{itemize}


\begin{acks}
 This section will be added for the camera-ready version.

\end{acks}


\bibliographystyle{ACM-Reference-Format}
\bibliography{acmart}

\end{document}
