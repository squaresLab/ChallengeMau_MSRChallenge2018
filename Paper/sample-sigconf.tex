\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables

\newcommand{\todo}[1]
  {{\scriptsize \textbf{\color{red} {#1}}}}


% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}



% DOI
\acmDOI{10.475/123_4}

% ISBN
\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[MSR'18]{Mining Software Repositories}{May 2018}{Gothenburg, Sweden}
\acmYear{2018}
\copyrightyear{2018}


\begin{document}
\title{Common Statement Kind Changes to Inform Automatic Program Repair}
%\titlenote{Produces the permission block, and
%  copyright information}
%\subtitle{Extended Abstract}
%\subtitlenote{The full version of the author's guide is available as
%  \texttt{acmart.pdf} document}


%\author{Mauricio Soto}
%\affiliation{%
%  \institution{Carnegie Mellon University}
%  \city{Pittsburgh}
%  \state{PA}
%  \postcode{15213}
%}
%\email{mauriciosoto@cmu.edu}

%\author{Claire Le Goues}
%\affiliation{%
%  \institution{Carnegie Mellon University}
%  \city{Pittsburgh}
%  \state{PA}
%  \postcode{15213}
%}
%\email{clegoues@cs.cmu.edu}

\author{Authors Available Post Double Blind Review}
\affiliation{%
  \institution{~~}
  \city{~~}
  \state{~~}
  \state{~~}
  \state{~~}
  \postcode{~~}
}
\email{  }



% The default list of authors is too long for headers.
%\renewcommand{\shortauthors}{B. Trovato et al.}


\begin{abstract}
The search space for automatic program repair approaches 
is vast and the search for mechanisms to help restrict this 
search are increasing.
We make a granular analysis based on statement kinds
to find which statements are more likely to be modified than others
when fixing an error.
We construct a corpus for analysis by 
delimiting debugging regions in the provided dataset and 
recursively analyze the
differences between the SST's associated with EditEvent's. 
We build a distribution of statement kinds with their corresponding
likelihood of being modified and we validate the usage of this 
distribution to guide the statement selection.
We then build association rules with different
confidence thresholds to describe statement kinds commonly modified
together for multi-edit patch creation. Finally
we evaluate association rule coverage over a held out test set
and find that when using a 95\% confidence threshold we
can create less and more accurate rules that fully cover 93.8\% of the testing instances.

\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011074.10011111.10011113</concept_id>
<concept_desc>Software and its engineering~Software evolution</concept_desc>
<concept_significance>100</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[100]{Software and its engineering~Software evolution}

\maketitle

\section{Introduction}

Bug repair is one of the most resource consuming tasks in 
software development~\cite{Weiss07,Tassey02,Britton13}.
In the last decade there has been increasing attention to 
techniques for automatic program repair (APR), which 
repair software bugs
automatically (e.g.~\cite{legoues12,kim2013,Weimer13,long16proph}).
One family of techniques follows a syntactic \emph{generate-and-validate}
approach (e.g.~\cite{legoues12,kim2013,Qi13TrpAutoR,xuan16}). These techniques
take as input a buggy program and a test suite containing
passing tests cases and failing test cases (which expose a defect).
The approaches then \emph{generate} 
patch candidates by applying and combining syntactic source transformations, and 
\emph{validate} these 
candidates against the test suite.

One challenge in this process is that the search space
of patch candidates is trivially infinite.  APR approaches thus restrict the
considered transformations, and use off-the-shelf fault
localization~\cite{legoues12} to reduce the number of locations considered for
editing. One problem is that 
statistical fault localization can often assign multiple statements
the same score~\cite{Jones02}. Even further, patches that require
more than one edit are commonly found in the real world and have
been vastly understudied. The need for mechanisms that help guide
the search space for multi-edit patches keeps growing.

Evidence shows that not all statements are modified
equally, and there are benefits from using history-based
data~\cite{kim2013,Soto18}. 
Therefore, we first analyze the most commonly 
modified statement kinds and evaluate how the 
fault localization process can benefit from assigning
a higher priority to the statements most commonly modified.
Second, we create association rules to describe relationships
between statement kinds commonly modified together to inform
the creation of multi-edit patch candidates. 

The MSR Challenge dataset~\cite{msr18challenge}
describes the actions performed
by developers when coding.
We mined changes performed in the debugging
process by delimiting debugging regions
and comparing the simplified syntax
trees (SST) of consecutive \textit{EditEvent's} using an 
state-of-the-art tree differencing tool~\cite{Pawlik16Apted}.
We then apply the association
rule learning algorithm Apriori~\cite{Agrawal94} to this corpus
to obtain relationships that describe what statement kinds
are edited commonly together.

We evaluate the level to which the rules can predict edits
by analyzing what sections of the events are covered by the 
learned association rules. 
Finally we analyze
the confidence level of the learned rules. 
We found that when we use a 95\% confidence
threshold, we can create association
rules that maintain the same flexibility as 
lower confidence thresholds, but increases
the accuracy of the created rules and decreases the number of rules
delimiting even more the search space of potential edits
performed by APR approaches to obtain successful patches.

The main contributions of this paper are the following:
\begin{itemize}
\item We \textbf{describe the distribution} of statement kinds being
modified by developers when fixing errors in source code. 

\item We \textbf{evaluate
the selection process} of statement kinds to be modified
to fix source code errors, using history-based data (49.54\% correctly guessed), compared against
an equally distributed random approach (5.11\% correctly guessed).

\item We \textbf{create association rules} that describe the behavior
of statement kinds that developers modify commonly together. 
We evaluate the expressive power of the learned rules, we find
that we can fully cover 93.8\% of the held-out events while maintaining
95\% confidence.

\end{itemize}

\section{Creating the Corpus}

We collect a corpus of events based in modifications 
developers perform when fixing an error from the MSR Challenge 
dataset~\cite{msr18challenge}. We then
use this corpus to find differences between consecutive events
and their corresponding Simplified Syntax Tree's (a snapshot of
the source code when the event took place) to create a distribution 
of statement kinds modified and association rules.

\subsection{Delimiting Debugging Regions}
\label{delimitDebugRegions}


The first step towards analyzing the debugging
process is identifying periods during which a 
developer is trying to fix a bug.
We delimit the debugging regions by starting at a point
in time $\delta$ where a \textit{TestRunEvent} is triggered and 
all test cases were run (test case execution
was not aborted) but one or more test cases failed, while in the 
previous \textit{TestRunEvent}, all tests passed.
The delimitation ends at a later point in time $\epsilon $ where 
$\epsilon $ is the closest \textit{TestRunEvent} later in
chronological order
where all test cases were ran and passed.

Figure~\ref{demarcations} shows an example of a time line
with only the \textit{TestRunEvent}s shown. ``P" represents a case
where all test cases passed; ``F" represents a case where one
or more test cases failed. The \texttt{debugging areas} are shown by
the brackets demarcated with ``DA". 
Note that between delimiting points $\delta$ and $\epsilon$ there can be 
several \textit{TestRunEvent's} with failing test cases, meaning
that the developer is actively debugging the error pointed out
by the failing test cases, but still has not been able to make
all test cases pass.

\begin{figure}[h]
\caption{Example of the process used to obtain the demarcation of
debugging areas. Events where all test cases pass (P) and
events where one or more tests fail (F) are analyzed to create
debugging areas (DA).}
\centering
\includegraphics[width=0.5\textwidth]{images/demarcations.png}
\label{demarcations}
\end{figure}

We identified 634 debugging sequences in the dataset 
provided by the MSR Challenge~\cite{msr18challenge}.
Within these debugging sequences, there are 1,251,334
unique events, from which 1,748 are \textit{EditEvent's}
that have associated a not-unknown Contexts, and therefore 
contain a valid Simplified Syntax Tree. These \textit{EditEvent's} 
are our primary subject of analysis.

\subsection{Simplified Syntax Tree Differencing}
\label{sstDiff}

Given two consecutive \textit{EditEvent's} within
a debugging region, we proceed to analyze the differences
between their two associated Simplified Syntax Trees (SST).
These correspond to the modifications the developer performed
while debugging.
We use the 
state-of-the-art tree differencing
tool APTED~\cite{Pawlik16Apted}, recursively converting the SST's into a
representation readable by APTED. 
Once we know which nodes were modified between \textit{EditEvent}s, we record
their corresponding statement types. 

\subsection{Statement Kind Distribution and Association Rule Creation}

We analyzed the distribution of statement kinds modified
in the corpus.  EditEvent's may modify more than one
statement kind, therefore the sum of these distribution
does not sum to 100\%.
From our 1,748 events, the most commonly modified statement
kinds are \texttt{ExpressionStatement}'s, modified in 1,272 (72.8\%) 
corpus events,
followed by \texttt{Assignment}'s modified in 1,186 
events (67.8\%). \texttt{VariableDeclaration}'s are modified in 1,071
events (61.3\%) and \texttt{ReturnStatement}'s in 714
events (40.8\%).\footnote{The full list of Statement kind
distribution of the corpus is located in the anonymous folder 
http://bit.ly/2EMBeh3} This confirms our initial hypothesis that
not all statement kinds are modified equally.


%Table~\ref{corpusDist} shows the top 10 most commonly modified 
%statement kinds
%in the corpus. The "Statement Kind" column shows the name of the 
%Statement kind being modified, the "Instance Precense" column
%shows the number of instances that modify the corresponding
%statement kind, and the "Percentage Presence" column shows 
%how many instances in the context of the entire corpus of 
%1,748 edits found in the debuggin areas modify the corresponding
%statement kind. For example, the first row states that
%an "ExpressionStatement" is modified in 1,272 EditEvent's from the
%1,748 that make up the entire corpus, therefore  it is modified
%in a 72,8\% of the corpus. EditEvent's may modify more than one
%statement kind.

%\begin{table}[]
%\centering
%\caption{Distribution of top 10 statement kinds modified by 
%developers when fixing an error in source code. }
%\label{corpusDist}
%\begin{tabular}{lrr}
%\toprule
%Statement Kind      & Instance Precense & Percentage Presence  \\
%\midrule
%ExpressionStatement & 1272 & 72.8\% \\
%Assignment          & 1186 & 67.8\% \\
%VariableDeclaration & 1071 & 61.3\% \\
%ReturnStatement     & 714  & 40.8\% \\
%IfElseBlock         & 657  & 37.6\% \\
%ForEachLoop         & 334  & 19.1\% \\
%ThrowStatement      & 181  & 10.4\% \\
%TryBlock            & 180  & 10.3\% \\
%UsingBlock          & 103  & 5.9\%  \\
%WhileLoop           & 86   & 4.9\%  \\
%\bottomrule
%\end{tabular}
%\end{table}

To collect rules for statement kinds that are modified together in
multi-edit patches, we use Apriori~\cite{Agrawal94} to create
association rules.
Association rule learning is a machine learning mechanism that identifies
relationships between objects in large datasets. 
Association rules are implications of the
form $X \implies Y$.
% antecedent->conequent is a known thing from logic
This learning mechanism takes into 
account \textit{Confidence} (a measure of the likelihood 
of the consequent section being present given the 
antecedent) and \textit{Support} (how frequently the set of
statement types in the rule occurs in the corpus).
Using our corpus of edits and their associated statement types, 
we create association rules that provide guidelines
for what statement types are modified together commonly. 

\section{Evaluation}
\label{eval}

We evaluate the modified statement kind selection process
by comparing a random selection against our History-based 
approach which gives higher priority to more commonly 
modified statement kinds. We then evaluate the association
rules by measuring the percentage of edit instances they correctly predict, or 
cover.

\subsection{History-Based Statement Selection}

To evaluate the distribution of the statement kinds modified
in the corpus, we compare against an equally distributed counterpart.
This simulates the process being used by most current APR approaches
where code is analyzed, using a fault localization 
mechanism, and an statement is randomly selected from the most
suspicious block.

We evaluate the process of selecting an statement kind informed by 
history-based data and check if it correctly predicts held-out 
instances.
We use 10 fold
cross validation to avoid training and testing on the same data.
For each fold, we
iterate through each EditEvent. For
the equally distributed approach, we randomly select one from 
all possible statement kinds and check if the selected
statement kind was modified in the EditEvent.
For our history-informed approach, we create a distribution
of the statement kinds modified from the complement of the fold, 
and select a statement
kind bounded by the associated distribution.
The more commonly modified statement
kinds are more likely to be chosen than the less commonly modified
statement kinds.

Table~\ref{10FoldEDvsHB} describes the results per each evaluated fold.
On average, 
the History-based approach is able to correctly guess one of the statement kinds
49.54\%. The Equally distributed approach
on average is only able to correctly guess 5.11\% of the time.
Two sample t-test indicates that the difference between the Equally distributed
and History-based approaches is statistically significant ($\alpha$ < 0.05).


\begin{table}[]
\centering
\caption{Evaluation of modified statement kind selection (Equally distributed vs History-based).}
\label{10FoldEDvsHB}
\begin{tabular}{l|rr|rr}
\toprule
   &   \multicolumn{2}{c|}{ Equally Distributed}   &   \multicolumn{2}{|c}{ History-Based} \\
\midrule
Fold  &   Count & Percentage  &  Count & Percentage  \\
\midrule
1&13&(7.39\%) & 72&(40.90\%) \\
2&4&(2.27\%) & 98&(55.68\%) \\
3&11&(6.25\%) & 86&(48.86\%) \\
4&7&(3.98\%) & 84&(47.72\%) \\
5&10&(5.68\%) & 92&(52.27\%) \\
6&7&(3.98\%) & 89&(50.57\%) \\
7&10&(5.68\%) & 92&(52.27\%) \\
8&8&(4.54\%) & 89&(50.57\%) \\
9&11&(6.25\%) & 96&(54.54\%) \\
10&9&(5.11\%) & 74&(42.04\%) \\
\bottomrule
Mean &9 & 5.11 & 87.2 & 49.54 \\
\bottomrule
Std Dev & 2.45 & 1.39 & 8.15 & 4.43 \\
\bottomrule

\end{tabular}
\end{table}



\subsection{Association Rule Coverage}

We next evaluate the effectiveness
of the association rules and their potential for use in building the
necessary edits to create successful multi-edit patches. We first
remove from our corpus the instances where the developers
performed a single edit. This removed 578 instances from the initial 1,748.
We perform this restriction because association rules
by definition require at least two elements: an antecedent
and a consequent. 

We then analyze how often the association rules can build the 
modifications in the events by using rule coverage. We use 10 fold cross validation to avoid training and testing in overlapping data.
First, we divide our corpus into 10 folds.
For each of the folds we perform the following actions:
the selected fold is used as testing data, and the
remaining nine folds are used to create association rules.
We then analyze how many of the events in the testing data
can be built from the association 
rules created from the remaining nine folds.
The learned rules can either cover all the edits in an event (the 
event is \emph{fully covered}), they can cover some of the edits in an 
event (the event is \emph{partially covered}), or they can cover none of
the edits in an event (the event is \emph{not covered}).

We show an example for further understanding of the coverage concept.
Table~\ref{rulesandinstances} shows three events. In 
our example Event~1 describes an event
where the developer modified an \texttt{Assignment}, 
a \texttt{ForLoop} and an \texttt{IfElseBlock}.
The rules shown in Table~\ref{rulesandinstances} represent rules created
from the remaining nine folds of the corpus. In our example: Rule~1 
indicates that when an \texttt{Assignment} and a \texttt{ForLoop} 
have been modified,
the next step is to modify an \texttt{IfElseBlock}.

\begin{table}[ht]
  \centering
  \caption{Events (top); learned association rules (bottom). \label{rulesandinstances}}{\small
\begin{tabular}{ll}
\toprule
\multicolumn{2}{c}{Events} \\
\midrule
1 & Assignment; ForLoop; IfElseBlock  \\
2 & Assignment; ForLoop; VariableDeclaration\\  
3 & Assignment; ForLoop;  ReturnStmt \\
\midrule
 \multicolumn{2}{c}{Association Rules} \\                     
\midrule
1 & Assignment $\wedge$ ForLoop $\rightarrow$ IfElseBlock \\  
2 & ForLoop $\rightarrow$ VariableDeclaration \\   
\bottomrule
\end{tabular}
}
\end{table}

In this example, Event~1 is fully covered since Rule~1 covers all
the edits in the event. For Event~2, Rule~1 does not apply, even though
Event~2 contains the antecedent of Rule~1, it does not contain the 
consequent, therefore the rule is discarded. Rule~2 does apply to 
cover the latter two
edits of Event~2,
therefore Event~2 is partially covered. Finally Event~3 is not covered,
even when some of its edits are contained in the antecedent of the rules,
there is no rule that would successfully predict the behavior of this 
event. 

Finally we performed this analysis at 3 different confidence thresholds
(90\%, 95\%, 100\%) to compare the accuracy of the rules against the 
number of rules that can be created with lower confidence. 
Higher confidence produces a small set of very
accurate rules, in which it is very likely that when the antecedent
is present the consequent will be present as well. But this approach
does not allow for much flexibility and the rules overfit to the corpus 
they were created from. Lower 
confidence creates a wide set of rules that are less accurate (if
the antecedent is present is does not necessarily mean that the 
consequent will be present as well) but more flexible. 

% CLG: I cut the reference to your prior paper because you used it broadly as
% though it were talking about all a priori learning and it's not, it's just
% about one use case (and not this one.)
For each confidence
threshold of 90\%, 95\%, and 100\%, we evaluated association rule coverage on a held out
dataset, using 
a standard 10 fold cross validation process. Finally, we aggregate the results
from all folds.

Figure~\ref{ruleEvaluation} shows our results for each of the 
different confidence thresholds as described below.

\vspace{1em}
\noindent\textbf{90\% confidence threshold:}
When evaluating the rules created under a 90\% threshold, 94.14\% of the 
events are fully covered, 1.87\% are partially covered, and 3.99\% are
not covered. When using this approach, an average of 2837.2 rules were
created for the 10 folds.

\vspace{1em}
\noindent\textbf{95\% confidence threshold:}
93.80\% of the testing instances are fully covered, 2.04\% are
partially covered, and 4.9\% are not covered. On, average 2134.3 rules
were created.

\vspace{1em}
\noindent\textbf{100\% confidence threshold:}
Finally, 35.85\% are fully
covered, 2.29\% are partially covered, and 72.80\% are not covered.
On average, 1484.6 rules were created when using this approach.

\vspace{1em}
We found that when creating association
rules with empirical data for automatic program repair, 95\% confidence appears
idea: 
it maintains very similar flexibility 
to the 90\% threshold with improved accuracy.
This also means that the number of rules created decreases, which helpfully
restricts 
the search space by restricting it to a smaller number of more accurate rules.
 

\begin{figure}[h]
\caption{The wide bars are described by the left Y axis. This represents
the percentage of instances covered
fully, partially or not by the association rules (higher full coverage is better). 
The thin bars are described by the right Y axis.
This represents the number of rules created for each confidence threshold 
(lower is better).
The bars from left to right describe the cross validation
results when using 90\%, 95\% and 100\% confidence thresholds correspondingly.}
\centering
\includegraphics[width=0.5\textwidth]{images/assocRuleEval.png}
\label{ruleEvaluation}
\end{figure}

\subsection{Threats to Validity}
\noindent\textbf{Internal validity:}
To tackle possible errors in our implementation and experiments, we release our code
to be reviewed by the other researchers.\footnote{All instruments
and source code can be found in the anonymous folder 
http://bit.ly/2EMBeh3} 
We also use 10 fold cross validation 
to reduce the risk of training and testing on the same data.  

\noindent\textbf{External validity:} 
It is possible 
that our results will not generalize to external datasets and to
real bug fixes. To mitigate this concern, we have created our corpus 
from the dataset made available to us by the MSR Challenge~\cite{msr18challenge}
which records the steps developers take while in the software development process.
This dataset is gathered from a diverse pool of volunteers with different 
backgrounds and levels of expertise.


\section{ Related Work} 
Soto et al.~\cite{Soto18} have created association rules to inform the APR
process basing their corpus in \textbf{mutation operators} taken from popular Github projects
written in Java using commit level granularity, 
different from our approach which creates association rules for \textbf{statement types} 
based in finer grained C\# code changes~\cite{msr18challenge}. Mutation operators help to 
inform what \textit{action} to perform next, while statement types help to inform what
object to modify next. 
Par~\cite{kim2013} describes an analysis of common changes
applied by humans when fixing errors and templates that can be mined from
their corpus. HDRepair~\cite{xuan16} 
uses fix history at a broader level
to assess patch suitability.
Zhong and Su~\cite{zhong15} conduct an empirical study on six popular Java 
projects analyzing the incidence of three mutation operators. 
Martinez and
Monperrus~\cite{martinez15} study mutation operator incidence across 
14 projects.
Prophet~\cite{long16proph} creates a
probabilistic model from 
the history of 8 different projects to rank candidate
patches.


\section{Conclusions}
\label{conclusions}

In this study we mined the dataset provided by the MSR Challenge~\cite{msr18challenge}
to create a corpus of highly granular edits performed by developers in the process
of fixing an error in source code. We identify debugging
regions and inspect the changes between the simplified 
syntax trees from each
of the \textit{EditEvent's} to create a corpus of 
events.
We analyze the distribution of commonly modified statement kinds and evaluate
how a search process bounded by this history can correctly guess (49.54\% of 
cases) what statement kind is modified to fix an error, as opposed to its
equally distributed counterpart (5.11\% of the cases).

We then create association rules to provide guidelines of what kinds of
statement to edit together to create successful multi-edit patches. 
We measure how many of the events
can be fully, partially, or not covered by the association rules; we find that
95\% appears to be the preferred confidence threshold.
We are able to fully cover 93.80\% of the events with a 95\% confidence threshold
which increases the accuracy of the rules created and decreases the 
number of rules, therefore helping delimit the vast search space 
in automatic program repair. These findings can be used to guide the 
creation of multi-edit patches in an APR context therefore delimiting
the vast search space and providing guidelines for properly selecting
statement kinds to modify based in the history of developer changes.


%Below are shown the top 10 rules created with a 95\% confidence, a full list can be 
%found online\footnote{the link will be made available after double blind
%review since it may uncover the identity of the authors}:
%\begin{itemize}
% \item ReturnStatement \& VariableDeclaration $\implies$ Assignment
% \item IfElseBlock \& VariableDeclaration $\implies$ Assignment
% \item ExpressionStatement \& VariableDeclaration $\implies$ Assignment
% \item VariableDeclaration $\implies$ Assignment
% \item IfElseBlock \& ExpressionStatement $\implies$ Assignment
% \item ExpressionStatement \& ReturnStatement $\implies$ Assignment
% \item IfElseBlock $\implies$ Assignment
% \item ReturnStatement $\implies$ Assignment
% \item IfElseBlock \& Assignment $\implies$ VariableDeclaration
% \item Assignment \& ReturnStatement $\implies$ VariableDeclaration
% \end{itemize}


\begin{acks}
 This section will be added for the camera-ready version.

\end{acks}


\bibliographystyle{ACM-Reference-Format}
\bibliography{acmart}

\end{document}
