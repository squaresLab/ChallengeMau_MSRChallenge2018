\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables

\newcommand{\todo}[1]
  {{\scriptsize \textbf{\color{red} {#1}}}}


% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}



% DOI
\acmDOI{10.475/123_4}

% ISBN
\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[MSR'18]{Mining Software Repositories}{May 2018}{Gothenburg, Sweden}
\acmYear{2018}
\copyrightyear{2018}


\begin{document}
\title{Common Statement Kind Changes and their Application in Automatic Program Repair}
%\titlenote{Produces the permission block, and
%  copyright information}
%\subtitle{Extended Abstract}
%\subtitlenote{The full version of the author's guide is available as
%  \texttt{acmart.pdf} document}


\author{Mauricio Soto}
\affiliation{%
  \institution{Carnegie Mellon University}
  \city{Pittsburgh}
  \state{PA}
  \postcode{15213}
}
\email{mauriciosoto@cmu.edu}

\author{Claire Le Goues}
\affiliation{%
  \institution{Carnegie Mellon University}
  \city{Pittsburgh}
  \state{PA}
  \postcode{15213}
}
\email{clegoues@cs.cmu.edu}


% The default list of authors is too long for headers.
%\renewcommand{\shortauthors}{B. Trovato et al.}


\begin{abstract}
Automatic Program Repair approaches usually depend on fault localization
mechanisms to find areas of code with a spectrum of suspiciousness to 
decide what statements of code to modify. As a result of this, there
are usually large blocks of code with the same suspiciousness and 
ultimately one statement is picked at random to be modified.
We make a higher level granularity analysis based on statement kinds
to find which statements are more likely to be modified than others
when fixing an error.
We construct a corpus for analysis by 
delimiting debugging regions in the provided dataset and 
recursively analyze the
differences between the SST's associatiated with EditEvent's. 
We build a distribution of statement kinds with their corresponding
likelihood of being modified and we validate the usage of this 
distribution to bound the statement selection.
We then build association rules with different
confidence thresholds to describe statement kinds commonly modified
together for multi-edit patch creation. Finally
we evaluate association rule coverage with 10 fold cross validation. 


\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011074.10011111.10011113</concept_id>
<concept_desc>Software and its engineering~Software evolution</concept_desc>
<concept_significance>100</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[100]{Software and its engineering~Software evolution}


\keywords{Debugging, Multi-edit automatic program repair}


\maketitle

\section{Introduction}
Error repair is one of the most resource consuming tasks in 
the software development process~\cite{Weiss07,Tassey02,Britton13}.
In the last decade there has been increasing attention to 
the concept of automatic program repair (APR). These are a 
series of approaches whose intent is to repair software
automatically (e.g.~\cite{legoues12,kim2013,Weimer13,long15SPR,long16proph,debroy10,perkins09,wei10}).
A well-known family of techniques follows the \emph{generate-and-validate}
approach (e.g., GenProg~\cite{legoues12}, 
Par~\cite{kim2013}, TrpAutoRepair~\cite{Qi13TrpAutoR}, HDRepair~\cite{xuan16}) 
which take as input a buggy program and a test suite containing
passing and failing test cases.
The passing test cases show the situations where the program  
behaves as intended, and the failing test cases show desired behavior
that is currently not implemented therefore exposing a defect.
The approaches then \emph{generate} variants of the buggy program
called patch candidates, and \emph{validate} these patch
candidates against the test suite with the intent of finding 
a patch candidate that passess all test cases in the test suite.

This process is particularly difficult because the search space
for creating patch candidates is infinite. To create a patch 
candidate the approach needs to pick a series of edits to apply.
An edit consists of a location in the source code,
and a mutation operator. The location describes the section
of the code that will be modified, while the mutation operator
outlines the type of edit that will be applied to the code.

Currently most APR approaches use fault localization mechanisms to 
narrow down which possible blocks of code are
more suspicious than others.  
These blocks of code are 
a set of several possible statements, all of which are assigned
the same likelihood of being selected for modification.
Ultimately one of them is selected at random as a starting point 
following the assumption that all statements within a block
have the same likelihood of containing the error.
We believe this selection could be improved by using
developer's
edit history. We therefore analyze the most commonly 
modified statement kinds and evaluate how the 
fault localization process can benefit from assigning
a higher priority to the statements most commonly modified.

After this first fault localization step, APR approaches 
commonly produce single edits per patch
candidate, even though multi-edit repairs are vastly common in the 
development process. One reason why this happens is because
there are time and hardware constraints in the APR process,
the search space is infinite, and there are not enough guidelines
to guide the search.
This study intents to contribute to
solve this problem by analyzing  
the kind of edits that can be applied together to form
multi-edit patches.

We mined changes performed by developers in the debugging
process by delimiting debugging regions in the dataset~\cite{msr18challenge} and comparing the simplified syntax
trees (SST) of consecutive \textit{EditEvent's} using an 
state-of-the-art tree differencing tool~\cite{Pawlik16Apted}.
We then gather the statement kinds edited in each of the 
differences between SST's and apply the association
rule learning algorithm Apriori~\cite{Agrawal94}
to obtain relationships that describe what statement kinds
are edited commonly together. This knowledge helps to 
bound the search space for automatic program repair approaches
in the effort of building multi-edit patches.

We evaluate the learned association rules by 
analyzing how many of the edits
in each event can be predicted by the association rules.
We categorize the level to which the rules can predict edits
analyzing what sections of the edits are covered by the 
learned association rules. 
We use 10 fold
cross validation to avoid training and testing in the same data.
Finally we analyze
the confidence level of the learned rules to a more granular 
level than performed before in the context of automatic
program repair. We found that when we use a 95\% confidence
threshold, we can create association
rules that maintain the same flexibility as previous
studies with lower confidence thersholds, but increases
the accuracy of the created rules and decreases the number of rules
delimiting even more the search space of potential edits
performed by APR approaches to obtain successful patches.

The main contributions of this paper are the following:
\begin{itemize}
\item \textbf{Describe and Evaluate Distribution} of statement kinds being
modified by developers to fix errors in source code by
analyzing the likelihood of correctly choosing what statement
kinds are modified using historic-based data.

\item \textbf{Create Association Rules} that describe the behavior
of statement kinds that developers modify commonly together when
trying to fix an error in source code.  

\item \textbf{Higher Granularity Evaluation} 
of confidence thresholds when creating
association rules in the context of multi-edit automatic
program repair using association rule coverage with 
10 fold cross validation.

\end{itemize}

\section{Gathering the Corpus}
\subsection{Delimiting Debugging Regions}
\label{delimitDebugRegions}
The first step towards being able to analyze the debugging
process developers go through is to be able to tell when
developers are trying to fix a bug. A very straightforward 
way to know when a developer is debugging is by keeping
record of when the developer triggers the debugger. An important
disadvantage of this approach is that 
developers often will try to fix errors in their code without the
need to trigger the debugger, and by following this approach,
these cases would not
be considered into the analysis.

In this study we take a broader approach to include the most 
instances where a developer is trying to fix an error.
We delimit the debugging regions by starting at a point
in time $\delta$ where a \textit{TestRunEvent} is triggered and 
all test cases were ran (the execution of test cases
was not aborted), but one or more test cases failed.
The delimitation ends at a point in time $\epsilon $ where $ \epsilon > \delta$ 
and where all test cases were ran and all test cases passed.
Figure~\ref{demarcations} shows an example of a time line
with only the \textit{TestRunEvent's}. The 
horizontal arrow represents a timeline showing
the events when a test suite was executed. "P" represents a case
where all test cases passed and "F" represents a case where one
or more test cases failed. The \texttt{debugging areas} are shown by
the brackets above with the "DA" initials. 


It is important to notice that the last \textit{TestRunEvent} before $\delta$ 
is always an event were all test cases passed, meaning that the
developer was not actively looking to fix a bug pointed out by the
failing test cases in the test suite. It is also worth noticing
that between delimiting points $\delta$ and $\epsilon$ there can be 
several \textit{TestRunEvent's} with failing test cases, meaning
that the developer is actively debugging the error pointed out
by the failing test cases, but still hasn't been able to make
all test cases pass.

\begin{figure}[h]
\caption{Example of the process used to obtain the demarcation of
debugging areas. Events where all test cases pass (P) and
events where one or more tests fail (F) are analyzed to create
debugging areas (DA).}
\centering
\includegraphics[width=0.5\textwidth]{images/demarcations.png}
\label{demarcations}
\end{figure}

We were able to obtain 634 debugging areas in the dataset 
provided by the MSR Challenge~\cite{msr18challenge}.
Within these 634 debugging areas, there are 1,251,334
different events, from which 1,748 are \textit{EditEvent's}
that have associated a not-unknown Contexts, and therefore 
contain a valid Simplified Syntax Tree. These will be 
our primary subject of analysis.

\subsection{Simplified Syntax Tree Differencing}
\label{sstDiff}
Once we have two consecutive \textit{EditEvent's} within
a debugging region, we procede to analyze the differences
between their two associated Simplified Syntax Trees (SST).
These are the modifications the developer performed
while debugging.
To perform the difference between trees we use the 
state-of-the-art tree differencing
tool APTED~\cite{Pawlik16Apted}.
We run recursively through each of the statements in the 
SST's creating a representation that can be understood by
APTED. This tool returns a list of mappings between
the SST of the previous \textit{EditEvent} and the SST
of the following \textit{EditEvent}. This mapping
accounts for node insertitons, deletions, and renaming.
Once we know which nodes were modified from one \textit{EditEvent}
to the following, we record their corresponding statement types.

\section{Instrument creation}

\subsection{Statement Kind Distribution}
\label{stmtKindDistribution}
Table~\ref{corpusDist} shows the top 10 most commonly modified 
statement kinds
in the corpus. The "Statement Kind" column shows the name of the 
Statement kind being modified, the "Instance Precense" column
shows the number of instances that modify the corresponding
statement kind, and the "Percentage Presence" column shows 
how many instances in the context of the entire corpus of 
1,748 edits found in the debuggin areas modify the corresponding
statement kind. For example, the first row states that
an "ExpressionStatement" is modified in 1,272 EditEvent's from the
1,748 that make up the entire corpus, therefore  it is modified
in a 72,8\% of the corpus. EditEvent's may modify more than one
statement kind.

\begin{table}[]
\centering
\caption{Distribution of top 10 statement kinds modified by 
developers when fixing an error in source code. }
\label{corpusDist}
\begin{tabular}{lrr}
\toprule
Statement Kind      & Instance Precense & Percentage Presence  \\
\midrule
ExpressionStatement & 1272 & 72.8\% \\
Assignment          & 1186 & 67.8\% \\
VariableDeclaration & 1071 & 61.3\% \\
ReturnStatement     & 714  & 40.8\% \\
IfElseBlock         & 657  & 37.6\% \\
ForEachLoop         & 334  & 19.1\% \\
ThrowStatement      & 181  & 10.4\% \\
TryBlock            & 180  & 10.3\% \\
UsingBlock          & 103  & 5.9\%  \\
WhileLoop           & 86   & 4.9\%  \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Association Rules}
\label{assocRules}
Association rule learning is a machine learning mechanism to identify
strong relationships between objects in large datasets. 
As a result, we obtain association rules, which are implications of the
form $X \implies Y$, where $X$ is called the antecedent and 
$Y$ is called the consequent.
We use the well-know association rule learning approach called 
Apriori~\cite{Agrawal94}. This learning mechanism takes into 
account \textit{Confidence} (a measure of the likelyhood 
of the consequent section being present given the 
antecedent) and \textit{Support} (how frequently the set of
statement types occurs in the corpus).

Using our corpus of edits and their associated statement types, 
we create Association Rules that provide guidelines
for what statement types are applied commonly together. This
information has great value in the area of Automatic 
Program Repair.

\section{Evaluation}
\label{eval}

\subsection{Historic-Based Statement Kind Selection}
To evaluate the distribution of the statement kinds being modified
in the corpus, we compare against an equally distributed counterpart.
This simulates the process being used by most current APR approaches
where blocks of code are analyzed using a fault localization 
mechanism and randomly selecting an statement from the most
suspicious block.

We use 10 fold
cross validation to avoid training and testing in the same data
when evaluating a selection bounded by the distribution
learned. For each of the folds we perform the following
tasks:
We iterate through each of the EditEvent's in the fold. For
the equally distributed approach, we randomly select one from 
the possible statement kinds and evaluate if the selected
statement kinds was modified in the EditEvent.
For our history-informed approach, we create a distribution
of the statement kinds being modified from the complement of the fold, 
and select a statement
kind bounded by the distribution 
learned from the complement. The more commonly modified statement
kinds are more likely to be chosen than the less commonly modified
statement kinds.

Table~\ref{10FoldEDvsHB} describes the results per each of the folds evaluated.
In average 
the Historic-Based approach is able to correctly guess one of  the statement kinds modified
in each of the instances of the fold being evaluated 49.54\% of the times. The Equally Distributed approach
in average is able to correctly guess one of the statement kinds being modified in 5.11\% 
of the evaluated instances of each fold.
Two sample t-test indicates that the difference between Equally Distributed and Historic-Based approaches are statistically significant ($\alpha$ < 0.05).


\begin{table}[]
\centering
\caption{Evaluation of modified statement kind selection (Equally distributed vs Historic-based) with 10 fold cross validation.}
\label{10FoldEDvsHB}
\begin{tabular}{l|rr|rr}
\toprule
   &   \multicolumn{2}{c|}{ Equally Distributed}   &   \multicolumn{2}{|c}{ Historic-Based} \\
\midrule
Fold  &   Count & Percentage  &  Count & Percentage  \\
\midrule
1&13&(7.39\%) & 72&(40.90\%) \\
2&4&(2.27\%) & 98&(55.68\%) \\
3&11&(6.25\%) & 86&(48.86\%) \\
4&7&(3.98\%) & 84&(47.72\%) \\
5&10&(5.68\%) & 92&(52.27\%) \\
6&7&(3.98\%) & 89&(50.57\%) \\
7&10&(5.68\%) & 92&(52.27\%) \\
8&8&(4.54\%) & 89&(50.57\%) \\
9&11&(6.25\%) & 96&(54.54\%) \\
10&9&(5.11\%) & 74&(42.04\%) \\
\bottomrule
Mean &9 & 5.11 & 87.2 & 49.54 \\
\bottomrule
Std Dev & 2.45 & 1.39 & 8.15 & 4.43 \\
\bottomrule

\end{tabular}
\end{table}



\subsection{Association Rule Coverage}
To evaluate the effectiveness
of the association rules and how these can build the
necesary edits to create successful patches we first
remove from our corpus the instances where the developers
performed a single edit. This is because association rules
by definition require at least two elements: an antecedent
and a consequent. The purpose of this study is to be able to
analyze and find relationships between two or more statement
kinds, therefore events where a single statement kind is altered
are not our subject of analysis.

We then perform a 10 fold cross validation over the corpus to 
check that the association rules are able to generalize to a
larger corpus. First, we divide our corpus into 10 folds.
For each of the folds we perform the following actions:
the selected fold is used as testing data, and the
remaining nine folds are used to create association rules.
We then analyze how many of the events in the test data
(the selected fold) can be built from the association 
rules created from the remaining nine folds.
The learned rules can either cover all the edits in an event (the 
event is \emph{fully covered}), they can cover some of the edits in an 
event (the event is \emph{partially covered}), or they can cover none of
the edits in an event (the event is \emph{not covered}).

We show an example for further understanding of the coverage concept.
Table~\ref{rulesandinstances} shows three events, each event is a 
highly granular step towards fixing an error in the source code. In 
our example Event~1 describes an step in the debugging process 
where the developer modified an Assignment, a ForLoop and an IfElseBlock.
The rules shown in Table~\ref{rulesandinstances} represent rules created
from the remaining nine folds of the corpus. In our example: Rule~1 
indicates that when an Assignment and a ForLoop have been modified,
the next step would be to modify an IfElseBlock.


\begin{table}[ht]
  \centering
  \caption{Events (top); learned association rules (bottom). \label{rulesandinstances}}{\small
\begin{tabular}{ll}
\toprule
\multicolumn{2}{c}{Events} \\
\midrule
1 & Assignment; ForLoop; IfElseBlock  \\
2 & Assignment; ForLoop; VariableDeclaration\\  
3 & Assignment; ForLoop;  ReturnStmt \\
\midrule
 \multicolumn{2}{c}{Rules} \\                     
\midrule
1 & Assignment $\wedge$ ForLoop $\rightarrow$ IfElseBlock \\  
2 & ForLoop $\rightarrow$ VariableDeclaration \\   
\bottomrule
\end{tabular}

}
\end{table}

In this example, Event~1 is fully covered since Rule~1 covers all
the edits in the event. For Event~2, Rule~1 does not apply, even though
Event~2 contains the antecedent of Rule~1, it does not contain the 
concequent, therefore the rule would not be successfully applied in a
real-life context. Rule~2 does apply to partially cover the latter two
edits of Event~2, but the first edit (Assignment) would not be covered
therefore Event~2 is partially covered. Finally Event~3 is not covered,
event when some of its edits are contained in the antecedent of the rules,
there is no rule that would successfully predict the behavior of this 
event. 

%There are 84 stmt kinds in this corpus


Finally we performed this analysis at 3 different confidence thresholds
(90\%, 95\%, 100\%) to compare the accuracy of the rules against the 
number of rules that can be created with lower confidence. 
There is a spectrum of confidence with which association rules
can be constructed. High confidence produces a small set of very
accurate rules, in which it is very likely that when the antecedent
is present, the concequent will be present as well. But this approach
does not allow for much flexibility and it overfits to the corpus 
they were created from. In the oposite side of the spectrum, low 
confidence, creates a wide set of rules that are less accurate (if
the antecedent is present is does not necessarily mean that the 
consequent will be present as well).All 
experiments have been performed with a support of 1\%, which 
means that the rules created must describe
at least 1\% of the instances of the corpus they are build from.

Previous studies show that in the 0\%-100\% spectrum, 
90\% confidence is an adecuate threshold 
for mantaining a balance between accuracy and flexibility when evaluated
discretely in intervals of 10 percentage points~\cite{Soto18}.
In this study we take it one step further and analyze the midpoint between 90\%
confidence and 100\% confidence to find if by increasing the confidence to a 5\%
we can obtain results with similar flexibility to rules created with 90\% 
confidence but the accuracy of the rules created with 100\% confidence.
For each confidence
threshold, we performed a standard 10-fold cross validation process with all the
events and all the rules for each fold and finally, we aggregate the results
from all folds.

Figure~\ref{ruleEvaluation} shows our results for each of the 
different confidence thresholds as described below.\footnote{All instruments
used to perform this evaluation can be found in the anonymouos folder 
https://www.dropbox.com/sh/lei59ywj1ok7gfk/AACc7JyJ-BcCa9-Kt1BqzGWja?dl=0}

\noindent\textbf{90\% confidence threshold:}
When evaluating the rules created under a 90\% threshold, 94.14\% of the 
events are fully covered, 1.87\% are partially covered, and 3.99\% are
not covered. When using this approach, an average of 2837.2 rules were
created for the 10 folds.

\noindent\textbf{95\% confidence threshold:}
When using a 95\% threshold, 93.80\% are fully covered, 2.04\% are
partially covered, and 4.9\% are not covered. In average 2134.3 rules
were created.

\noindent\textbf{100\% confidence threshold:}
Finally, when using a 100\% confidence threshold, 35.85\% are fully
covered, 2.29\% are partiallly covered, and 72.80\% are not covered.
In average 1484.6 rules were created when using this approach.

We found that when creating association
rules with empirical data for automatic program repair, 95\% confidence
is an even better threshold than 90\% because it maintains very similar flexibility 
to the 90\% threshold but improves the accuracy by increasing the confidence. 
This also means that the number of rules created decreases, which improves
the search space by restricting it to a smaller number of more accurate rules.
 

\begin{figure}[h]
\caption{The wide bars are described by the left Y axis. This represents
the percentage of instances covered
fully, partially or not by the association rules when using 10 fold
cross validation (higher full coverage is better). 
The thin bars are described by the right Y axis.
This represents the number of rules created for each confidence threshold 
(lower is better).
The bars from left to right describe the cross validation
results when using 90\%, 95\% and 100\% confidence thresholds correspondingly.}
\centering
\includegraphics[width=0.5\textwidth]{images/assocRuleEval.png}
\label{ruleEvaluation}
\end{figure}

\section{Threats to Validity / Related Work} \label{threatsVal}

\noindent\textbf{Internal validity:}
To tackle possible errors in our implementation and experiments, we release our code
to be reviewed by the other researchers. We also use 10-fold cross validation 
when evaluating the association rules
to reduce the risk of training and testing on the same data.  

\noindent\textbf{External validity:} 
It is possible 
that our results will not generalize to external datasets and to
real bug fixes. To mitigate this concern, we have created our corpus 
from the dataset made available to us by the MSR Challenge~\cite{msr18challenge}
which records the steps developers take while in the software development process.
This dataset is gathered from a diverse pool of volunteers with different 
backgrounds and levels of expertise.

Soto et al.~\cite{Soto18} have created association rules to inform the APR
process basing their corpus in \textbf{mutation operators} taken from popular Github projects
written in Java using commit level granularity, 
different from our approach which creates association rules for \textbf{statement types} 
based in finer grained C\# code changes~\cite{msr18challenge}. Mutation operators help to 
inform what \textit{action} to perform next, while statement types help to inform what
object to modify next. 

Martinez and
Monperrus~\cite{martinez15} study mutation operator incidence across 
14 projects. Par~\cite{kim2013} describes an analysis of common changes
applied by humans when fixing errors and templates that can be mined from
their corpus. HDRepair~\cite{xuan16} 
uses fix history at a broader level
to assess patch suitability.


\section{Conclusions}
\label{conclusions}
In this study we mined the dataset provided by the MSR Challenge~\cite{msr18challenge}
to create a corpus of highly granular edits performed by developers in the process
of fixing an error in the source code. To be able to do this, we identify debugging
regions taking into account the points in time when a test suite fails and after a 
series of events all tests pass. We then inspect the changes between the simplified 
syntax trees from each
of the \textit{EditEvent's} within the debugging regions to create a corpus of 
events (steps developers take towards fixing an error in the source code).

We create a distribution of commonly modified statement kinds and evaluate
how a search process bounded by this history can correctly guess (49.54\% of 
cases) what statement kind is modified to fix an error, as opposed to its
equally distributed counterpart (5.11\% of the cases).
We then create association rules to be able to obtain guidelines of what kinds of
statement to edit together. This lessens an important gap of knowledge existing
in automatic program repair, which is necessary to restrict the search space of 
possible edits to perform to create successful multi-edit patches.

We create these association rules using three different confidence thresholds and
we evaluate them using 10 fold cross validation. We measure how many of the events
can be fully, partially, or not covered by the association rules.
Our findings indicate that 95\% is a better confidence threshold as 
opposed to 90\% as presented in previous studies~\cite{Soto18}. 
We are able to fully cover 93.80\% of the events with a 95\% confidence threshold
while increasing the accuracy of the rules created and decreasing the 
number of rules, therefore restricting the search space in automatic program repair.


%Below are shown the top 10 rules created with a 95\% confidence, a full list can be 
%found online\footnote{the link will be made available after double blind
%review since it may uncover the identity of the authors}:
%\begin{itemize}
% \item ReturnStatement \& VariableDeclaration $\implies$ Assignment
% \item IfElseBlock \& VariableDeclaration $\implies$ Assignment
% \item ExpressionStatement \& VariableDeclaration $\implies$ Assignment
% \item VariableDeclaration $\implies$ Assignment
% \item IfElseBlock \& ExpressionStatement $\implies$ Assignment
% \item ExpressionStatement \& ReturnStatement $\implies$ Assignment
% \item IfElseBlock $\implies$ Assignment
% \item ReturnStatement $\implies$ Assignment
% \item IfElseBlock \& Assignment $\implies$ VariableDeclaration
% \item Assignment \& ReturnStatement $\implies$ VariableDeclaration
% \end{itemize}


\begin{acks}
 This section will be added for the camera-ready version.

\end{acks}


\bibliographystyle{ACM-Reference-Format}
\bibliography{acmart}

\end{document}
